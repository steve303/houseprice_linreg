---
title: "House Price Modeling in King County, WA"
author: "Steve Su, Pierson Wodarz"
date: "7/20/2020"
output:
  html_document: 
    theme: readable
    toc: yes  
  pdf_document: default
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80)
library(knitr)
opts_chunk$set(cache = TRUE, autodep = TRUE)
```

## Introduction
In this study we will investigate sales data for houses in King County, WA (which includes Seattle) sold between May 2014 and May 2015. In particular, we are concerned with the sales prices of these homes and identifying the factors contributing to the sale price such that it becomes possible to predict the price of a given house. 

This is useful to the authors as it can be used for personal decisions when moving to Seattle and purchasing a home. Additionally, it allows the authors to validate or compare to other listed estimates and prices, such as the list price of the home or estimates like the Zillow Zestimate or Redfin Estimate. For example, [Zillow's CEO sold his home for less than 40% of the Zillow estimate](https://www.geekwire.com/2016/zillow-ceo-spencer-rascoff-sold-home-40-less-zestimate-1-75-million/), underscoring the importance of accurate predictions and the potential for improvement in existing systems. Finally, even if the numerical values for the predictors are no longer valid, this study will allow the authors to identify an appropriate form of the model that can be trained on more recent data (house prices may have risen over the previous years, for example).

The data for this study was sourced from [Kaggle](https://www.kaggle.com/swathiachath/kc-housesales-data). The data set contains records of house sales in King County, WA, with a total of 21,597 observations and 21 variables. As our goal is to model the price of houses, we will treat the price variable as our response, and the other variables as potential predictors. These variables include numeric data types such as number of bathrooms/bedrooms, square footage of living space/basement, and number of floors. Additionally, the predictor variables include categorical variables such as zipcode and waterfront property or not.

In this study we will utilize several regression methods for modeling the data. 

## Methods
### Loading data
First, we load the data stored in [`kc_house_data.csv`](`kc_house_data.csv`) for the values of the predictors and the response. .
```{r message=FALSE, warning=FALSE}
library(readr)
house_data = read_csv("kc_house_data.csv")
```

### Data Pre-Processing

Upon receiving the data we removed several columns which we believed were not benefical as predictors.  These were the date, id number and views.  Since we wanted the model to be general, not influenced to the specific dates of this data set, we decided to remove the date column.  For obvious reasons we removed id number as this just used for tracking within the data set.  We also removed views because there seemed to be a discrepancy with its description.  The variable Views was described as the number of showings the house had, however its value is capped at four.  It's unusual for houses to have no more than four showings.  Because of this confusion we dropped it as one of the predictors. 

When importing the data set all variables were either a float or integer.  In order to denote which ones we wanted to be categorical we assigned them as factor variables.  These included, zipcode, waterfront, and yr_rennovated.  Lastly, we checked the data set for missing values but found none.  

### Initial Review of Data

Before any model building was done we performed an exploratory data analysis to help summarize the data to help provide insights which could be used during the model building phase of the project.  By constructing box plots of the predictors we made a decision to convert the yr_renovated predictor from a numeric data type to a boolean.  The original data had 0 if the property was not rennovated and year of rennovation (i.e. 2015) if the property was rennovated.  From observation of the box plot, the data was essentially a boolean data type so we converted it to one.  

A pairs plot was also constructed to gain insight between the relation of each predictor to the repsonse.  For example, We noticed that some predictors, such as bedrooms, seemed to have a positive exponential relation with the response. In contrast, sqft_lot seemed to have a negative exponential relation. The lat and long variables appeared to have some form of a polynomial relation.  Lastly, several predictors, such as grade and sqft_living, showed somewhat of a linear relation to the response.  This insight will be helpful when deciding how a predictor might be transformed when constructing the model.  

Additionally, a correlation table of all the predictors were made to get a sense of colinearity.  Some of the higher correlations found, > 0.70, were between the variables sqft_living:sqft_living15 and sqft_lot:sqft_lot15.  The difference between the two is sqft_living is the original square footage before 2015 and sqft_living15 is the square footage after 2015.  If there was an increase after 2015 this meant that there was square footage added to the house due to an expansion.  A majority of the houses do not have increases in square footage so there is high colinearity.  One could argue that the pre 2015 square footage data could be left out since it is already coded in the sqft_living15 variable.  However, we decided to leave it in and see how backward selection might treat them.  While the varible sqft_basement did not have a high correlation number we found that it had a perfect correlation to the other variables and was later removed.  We discovered this when an error was flagged when fitting the model using the lm() function.  There was a singularity error with the sqft_basement varible.  To further investigate, we fit a model using sqft_basement as the response with the remaining predictors and got a R squared value of one.  This shows that sqft_basement is actually made up of some combination the predictors itself.  

### Model Building
First, we randomly split this into a training data set (80% of the data) and a testing data set (20%) to use as a selection criteria. 
```{r}
set.seed(19940627)
house_idx = sample(nrow(house_data), as.integer(nrow(house_data) * 0.80))
house_trn = house_data[house_idx, ]
house_tst = house_data[-house_idx, ]
house_data = house_data[ ,-c(1, 2, 10, 14)] # Remove irrelevant variables (including id, date, view, sqft_basement)
house_data$yr_renovated = ifelse(house_data$yr_renovated == 0, 0, 1) #change yr_rennovated to boolean data type
house_data$yr_renovated = as.factor(house_data$yr_renovated)
house_data$waterfront = as.factor(house_data$waterfront)
house_data$zipcode = as.factor(house_data$zipcode)
```

We write a function that will return various selection metrics to be used as we progress through our model building.
```{r}
sel_crit = function(model, data) {
  n = nrow(data)
  LOOCV_RMSE = sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
  RMSE =  sqrt((1 / n) * sum((predict(model, newdata = data) - data$price) ^ 2))
  adjR2 = summary(model)$adj.r.squared
  AIC = extractAIC(model)[2]
  num_p = length(coef(model)) - 1
  data.frame(LOOCV_RMSE = LOOCV_RMSE, RMSE = RMSE, adjR2 = adjR2, AIC = AIC, num_p = num_p)
}
```


## Results

## Discussion

## Appendix